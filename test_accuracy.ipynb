{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from utils.evaluation import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "data_path=\"/home/emartini/nas/MAEVE/dataset/panoptic-toolbox/trtpose3D/\"\n",
    "result_path=\"/home/emartini/nas/MAEVE/dataset/panoptic-toolbox/results/\"\n",
    "date = datetime.fromtimestamp(time.time()).strftime('%Y_%m_%d_%H_%m')\n",
    "CONTINUOUS_STATE_PARTS = [\n",
    "            \"nose\", \"left_ear\", \"right_ear\", \"left_shoulder\", \"right_shoulder\", \n",
    "            \"left_elbow\", \"right_elbow\", \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\", \"left_knee\", \n",
    "            \"right_knee\", \"left_ankle\", \"right_ankle\", \"neck\"]\n",
    "\n",
    "# Load the json for comparison\n",
    "mapping = [12, 7, 10, 4,  5, 9, 6, 8, 11, 3, 14, 13]\n",
    "header = [\"frame_id\"]+[CONTINUOUS_STATE_PARTS[m] for m in mapping]\n",
    "\n",
    "people = {  \"170915_office1\": 1,\n",
    "            \"161029_tools1\": 2,\n",
    "            \"161029_build1\": 2,\n",
    "            \"160422_ultimatum1\": 7,\n",
    "            \"170407_haggling_a1\": 3,\n",
    "            \"161029_sports1\": 2, # Not working\n",
    "            \"160906_band4\": 3 # Not working\n",
    "        }\n",
    "\n",
    "# HOTA parameters\n",
    "step_min = 0.20\n",
    "step_max = 0.50\n",
    "step_size = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170407_haggling_a1\n",
      "cams: [6]\n",
      "befine\n",
      "pre-process time: 15.56 s\n",
      "process time: 27.42 s\n",
      "cometh\n",
      "pre-process time: 16.41 s\n",
      "process time: 39.34 s\n",
      "  Cams Aggregator  LocA  DetA  AssA  HOTA\n",
      "0    6     befine  81.9  41.0  14.6  24.3\n",
      "1    6     cometh  83.6  50.1  77.8  62.4\n",
      "cams: [7]\n",
      "befine\n",
      "pre-process time: 15.45 s\n",
      "process time: 38.49 s\n",
      "cometh\n",
      "pre-process time: 16.95 s\n",
      "process time: 40.91 s\n",
      "  Cams Aggregator  LocA  DetA  AssA  HOTA\n",
      "2    7     befine  79.0  35.5   6.9  15.6\n",
      "3    7     cometh  79.8  45.4  58.3  51.2\n",
      "cams: [8]\n",
      "befine\n",
      "pre-process time: 19.13 s\n",
      "process time: 43.43 s\n",
      "cometh\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m GT \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path,sequence_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.gt.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 38\u001b[0m     ground_truth \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m ground_truth:\n\u001b[1;32m     40\u001b[0m     GT[frame[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m frame        \n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compared methods\n",
    "from multiprocessing import Process, Manager\n",
    "manager = Manager()\n",
    "\n",
    "# cameras_list = [[6,7],[6,7,8],[6,7,8,9],[6,7,8,9,10]]\n",
    "# cameras_list = [[6,7],[6,8],[6,9],[6,10],[7,8],[7,9],[7,10],[8,9],[8,10],[9,10]]\n",
    "\n",
    "# All the combinantions\n",
    "cameras_list = [[6], [7], [8], [9], [10]] +\\\n",
    "                generate_combinations([6,7,8,9,10],2)+\\\n",
    "                generate_combinations([6,7,8,9,10],3)+\\\n",
    "                generate_combinations([6,7,8,9,10],4)+\\\n",
    "                generate_combinations([6,7,8,9,10],5)\n",
    "\n",
    "# cameras_list = [[6], [7], [8], [9], [10]]\n",
    "# cameras_list = generate_combinations([6,7,8,9,10],5)\n",
    "# cameras_list = [[8]]\n",
    "methods = [\"befine\",\"cometh\"] # \"befine\",\"cometh\"\n",
    "sequences = [\"170407_haggling_a1\"] #\"170915_office1\",\"161029_tools1\",\"161029_build1\",\"161029_sports1\"] # ,\"160422_ultimatum1\",\"170407_haggling_a1\",\n",
    "version = \"3\"\n",
    "\n",
    "result = []\n",
    "for sequence_name in sequences:\n",
    "    print(sequence_name)\n",
    "    for cameras in cameras_list:\n",
    "        print(\"cams:\",cameras)\n",
    "        \n",
    "        # Skip the tools cameras 8,9,10:\n",
    "        if  sequence_name == \"161029_tools1\" and any(e in [7,8,9,10] for e in cameras):\n",
    "            continue\n",
    "        \n",
    "        for method in methods:\n",
    "            print(method)\n",
    "            preprocess_time = time.time()\n",
    "            # Load ground truth\n",
    "            GT = {}\n",
    "            with open(os.path.join(data_path,sequence_name+\".gt.json\"), \"r\") as f:\n",
    "                ground_truth = json.load(f)\n",
    "            for frame in ground_truth:\n",
    "                GT[frame[\"timestamp\"]] = frame        \n",
    "\n",
    "            # Load dut file\n",
    "            DUT = {}\n",
    "            with open(os.path.join(data_path,sequence_name+\".\"+ method+\".\" +\".\".join(map(str, cameras)) +\".json\" ), \"r\") as f:\n",
    "                file = json.load(f)\n",
    "            for frame in file:\n",
    "                DUT[frame[\"timestamp\"]] = frame    \n",
    "\n",
    "            ## Build the triple nested list (shape: n_frames, n_people, n_joints, 3) and IDs (shape: n_frames, n_people)\n",
    "            # union of both gt and dut            \n",
    "            ids = list(GT.keys())\n",
    "            for frame_dut in list(DUT.keys()):\n",
    "                if frame_dut not in ids:\n",
    "                    ids.append(frame_dut) \n",
    "                    \n",
    "            predicted_keypoints = []\n",
    "            predicted_ids = []\n",
    "            ground_truth_keypoints = []\n",
    "            ground_truth_ids = []\n",
    "            for id in sorted(ids):\n",
    "                predicted_keypoints_per_frame = []\n",
    "                predicted_ids_per_frame = []\n",
    "                ground_truth_keypoints_per_frame = []\n",
    "                ground_truth_ids_per_frame = []\n",
    "                if id in GT.keys():\n",
    "                    for pp in GT[id]['continuousState']:\n",
    "                        s = np.array([ [np.nan,np.nan,np.nan] if not f else f for f in pp])\n",
    "                        s = s[mapping,:]\n",
    "                        ground_truth_keypoints_per_frame.append(s)\n",
    "                    for i in GT[id]['track_ids']:\n",
    "                        ground_truth_ids_per_frame.append(i)\n",
    "                if id in DUT.keys():\n",
    "                    for i,pp in enumerate(DUT[id]['continuousState']):\n",
    "                        s = np.array([ [np.nan,np.nan,np.nan] if not f else f for f in pp])\n",
    "                        s = s[mapping,:]\n",
    "                        if not np.isnan(s).all():\n",
    "                            predicted_keypoints_per_frame.append(s)\n",
    "                    # This doesn't work for cameras\n",
    "                    for I in DUT[id]['track_ids']:\n",
    "                        predicted_ids_per_frame.append(I)\n",
    "                \n",
    "                predicted_keypoints.append(predicted_keypoints_per_frame)\n",
    "                predicted_ids.append(predicted_ids_per_frame)\n",
    "                ground_truth_keypoints.append(ground_truth_keypoints_per_frame)\n",
    "                ground_truth_ids.append(ground_truth_ids_per_frame)\n",
    "\n",
    "            preprocess_time = time.time() - preprocess_time\n",
    "            print(\"pre-process time:\",round(preprocess_time,2),\"s\")\n",
    "            # Compute HOTA\n",
    "            process_time = time.time()\n",
    "            res = manager.list()\n",
    "            procs = []\n",
    "            step = step_min\n",
    "            thread_id = 0\n",
    "            while step <= step_max:\n",
    "                proc = Process(target=hota_par, args=(res,thread_id,predicted_keypoints, predicted_ids, ground_truth_keypoints, ground_truth_ids,step))\n",
    "                procs.append(proc)\n",
    "                proc.start()\n",
    "                step += step_size\n",
    "                thread_id += 1\n",
    "\n",
    "            for proc in procs:\n",
    "                proc.join()\n",
    "            \n",
    "            process_time = time.time() - process_time\n",
    "            print(\"process time:\",round(process_time,2),\"s\")\n",
    "            \n",
    "            res = list(res)\n",
    "            # row = [sequence_name,len(cameras),method] + list(np.nanmean(np.array(res),0)[1:])\n",
    "            row = [sequence_name,people[sequence_name],len(cameras),\",\".join(map(str, cameras)),method] + np.round(list(100*np.nanmean(np.array(res),0)),1).tolist()\n",
    "            result.append(row)\n",
    "            \n",
    "        header_stats = [\"Sequence\", \"#People\",\"#Cams\",\"Cams\", \"Aggregator\",  \"LocA\", \"DetA\", \"DetPR\", \"DetRE\", \"AssA\",\"AssPR\",\"AssRE\", \"HOTA\"]\n",
    "        df = pd.DataFrame(result,columns=header_stats)\n",
    "        print(df[[\"Cams\", \"Aggregator\", \"LocA\", \"DetA\", \"AssA\",\"HOTA\"]].tail(2).to_string())\n",
    "df.to_csv(os.path.join(result_path,date+\"_\"+sequence_name+'_V'+version+'.csv'), index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
